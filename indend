from datetime import datetime
from airflow import DAG
import os
import sys
import yaml
from airflow.providers.google.cloud.operators.bigquery import (
    BigQueryInsertJobOperator,
)
from airflow.providers.google.cloud.hooks.bigquery import BigQueryHook
from airflow.operators.python import PythonOperator
from airflow.operators.dummy import DummyOperator
from functools import partial
from datetime import datetime, timedelta
from airflow.utils.trigger_rule import TriggerRule
from airflow.sensors.sql import SqlSensor

BASE_DIR = "/home/airflow/gcs/dags/vz-it-gudv-dtwndo-0"
sys.path.append(f"{BASE_DIR}/nqes_site/python")


#from vz_de_common_observability_integration import publishLog, create_do_dict
from DO_utils import publishLog, create_do_dict

project = os.environ['GCP_PROJECT']
with open(f"{BASE_DIR}/nqes_site/config/base_config.yml", 'r') as file:
    base_config = yaml.full_load(file)

with open(f"{BASE_DIR}/nqes_site/config/gudv_nqes_site.yml", 'r') as file:
    dag_config = yaml.full_load(file)

config_values = {}


filtered_base_dict = dict(filter(lambda elem: elem[0] == project, base_config.items()))
filtered_dict = dict(filter(lambda elem: elem[0] == project, dag_config.items()))

if len(filtered_base_dict) > 0:
    base_value = filtered_base_dict[project][0]
    config_values = {**config_values, **base_value}
else:
    print("No config found exiting...")
    sys.exit(-1)
if len(filtered_dict) > 0:
    app_value = filtered_dict[project][0]
    config_values = {**config_values, **app_value}
else:
    print("No config found exiting...")
    sys.exit(-1)

GCP_PROJECT_ID = config_values['gcp_project']
bq_connection_id = config_values['google_cloud_conn_id']
region = config_values['region']
DAG_ID = config_values['dag_id']
base_directory = config_values['base_directory']
env = config_values['env']
dataset_id = config_values['dataset_id']
stored_proc = config_values['stored_proc']
table_name = config_values['table_name']
schedule_interval = config_values['schedule_interval']
failure_email_alert_distro = config_values['failure_email_alert_distro']

process_date = '{{ data_interval_end.strftime("%Y-%m-%d") }}'
trans_date = '{{ data_interval_end.subtract(days=1).strftime("%Y-%m-%d") }}'


default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(year=2025, month=4, day=30, hour=9, minute=00),
    'email': [failure_email_alert_distro],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=3)
}

dag = DAG(
    dag_id=DAG_ID,
    schedule_interval=schedule_interval,
    catchup=True,
    default_args=default_args,
    description='This DAG call Stored Procedure',
    concurrency=int(config_values['concurrency']),
    max_active_runs=int(config_values['max_active_runs']),
    tags=["dtwin","nw_research_assistant","network_genie","nqes_site"]
)


do_dict = create_do_dict(config_values)

start = DummyOperator(task_id='start',
                      dag=dag,
                      on_success_callback=partial(publishLog, "PROGRESS", do_dict),
                      on_failure_callback=partial(publishLog, "FAILURE", do_dict))


#Source Data Check Logic with Retry
def check_src_data(**kwargs):
    hook = BigQueryHook(gcp_conn_id=bq_connection_id, location=region)

    # Queries for the three source tables
    queries = {
        "fjpv": f"""
            SELECT COUNT(*) AS row_cnt
            FROM {fjpv_src_tbl}
            WHERE process_nm = 'ETL_NTWK_DMA_NQES_cell_gudv'
            AND status = 'COMPLETE'
            AND run_dt = '{kwargs["ds"]}'
        """,
        "enodeb_gudv": f"""
            SELECT COUNT(*) AS row_cnt
            FROM {enodeb_gudv_tbl}
            WHERE process_nm = 'ETL_enodeb_gudv_PROCESS'
            AND status = 'COMPLETE'
            AND run_dt = '{kwargs["ds"]}'
        """,
        "cell_gudv": f"""
            SELECT COUNT(*) AS row_cnt
            FROM {cell_gudv_tbl}
            WHERE process_nm = 'ETL_cell_gudv_PROCESS'
            AND status = 'COMPLETE'
            AND run_dt = '{kwargs["ds"]}'
        """,
    }

    retry_interval = 180  # 15 minutes
    max_attempts = 2  # 3 hours total (15 * 15 minutes)
    attempt = 0

    while attempt < max_attempts:
        print(f"Attempt {attempt + 1} of {max_attempts}")

        all_ready = True
        for table_name, query in queries.items():
            print(f"Executing query for {table_name}:\n{query}")
            result = hook.get_records(sql=query)
            count = int(result[0][0]) if result else 0
            print(f"{table_name} Result Count: {count}")

            if count == 0:
                all_ready = False
                break

        if all_ready:
            print("All source tables are ready for processing.")
            return True

        print("Data not available, retrying in 15 minutes...")
        time.sleep(retry_interval)
        attempt += 1

    # If we reached here, it means 3 hours passed without data
    raise AirflowException("Source data not available. Failing DAG execution.")

source_data_check = PythonOperator(
    task_id='source_data_check',
    python_callable=check_src_data,
    provide_context=True,
    retries=0,
    dag=dag,
)

call_nqes_site_sp = BigQueryInsertJobOperator(
        task_id="call_nqes_site_sp",
        dag=dag,
        gcp_conn_id=bq_connection_id,
        configuration={
                         "query": {
                              "query": f"CALL {dataset_id}.{stored_proc}('{trans_date}','{process_date}')",
                              "useLegacySql": False,
                              }
                         },
        on_failure_callback=partial(publishLog, "FAILURE", do_dict)

)
end = DummyOperator(task_id='end',
                    dag=dag,
                    on_success_callback=partial(publishLog, "SUCCESS", do_dict),
                    on_failure_callback=partial(publishLog, "FAILURE", do_dict))




start  >>source_data_check >> call_nqes_site_sp >> end
